{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ⬛ SET‑UP ⬛\n",
    "!pip install -q datasets transformers evaluate\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (AutoTokenizer, DataCollatorForLanguageModeling,\n",
    "                          AutoModelForMaskedLM, TrainingArguments, Trainer)\n",
    "import evaluate, json, os, re, itertools, random\n",
    "\n",
    "CORPUS_PATH = \"data/raw/toy_lang_dataset/text_corpus.txt\"   # swap to bigger corpus\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "# ⬛ LOAD TEXT ⬛\n",
    "with open(CORPUS_PATH) as f:\n",
    "    lines = [l.strip() for l in f if l.strip()]\n",
    "ds = Dataset.from_dict({\"text\": lines}).train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=128)\n",
    "ds = ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# ⬛ MODEL ⬛\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm_probability=0.15)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"models/vocab\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# ⬛ PERPLEXITY ⬛\n",
    "perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "ppl = perplexity_metric.compute(model_id=\"models/vocab\", input_texts=lines[:100])[\"perplexity\"]\n",
    "print(\"Perplexity:\", ppl)\n",
    "with open(\"results/vocab_perplexity.txt\", \"w\") as f:\n",
    "    f.write(f\"{ppl:.2f}\")\n",
    "\n",
    "# ⬛ GENERATE SIMPLE WORD‑LIST ⬛\n",
    "def get_vocab(model, tokenizer, top_k=100):\n",
    "    # sort by token frequency in tokenizer vocab indexes 999‑.. gives real sub‑words\n",
    "    tokens = tokenizer.get_vocab()\n",
    "    most_common = sorted(tokens.items(), key=lambda x: x[1])[:top_k]\n",
    "    return [tok for tok, _ in most_common]\n",
    "print(get_vocab(model, tokenizer)[:20])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

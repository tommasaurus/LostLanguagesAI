{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ⬛ SET‑UP ⬛\n",
    "!pip install -q datasets transformers sacrebleu\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
    "                          TrainingArguments, Trainer, DataCollatorForLanguageModeling)\n",
    "import sacrebleu, json, random, re, torch\n",
    "\n",
    "CORPUS_PATH = \"data/raw/toy_lang_dataset/text_corpus.txt\"\n",
    "MODEL_NAME = \"gpt2\"             # small; good for quick tests\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token   # GPT2 has no pad\n",
    "\n",
    "# ⬛ LOAD & BUILD PROMPTS ⬛\n",
    "def wrap(line):\n",
    "    return f\"<prompt>Generate a culturally relevant dialogue:\\n{line}\\n<end>\"\n",
    "with open(CORPUS_PATH) as f:\n",
    "    prompts = [wrap(l.strip()) for l in f if l.strip()]\n",
    "ds = Dataset.from_dict({\"text\": prompts}).train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=128)\n",
    "ds = ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# ⬛ MODEL ⬛\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"models/gpt2\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=20,\n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# ⬛ BLEU ⬛\n",
    "def eval_bleu():\n",
    "    preds, refs = [], []\n",
    "    for ex in ds[\"test\"]:\n",
    "        input_ids = tokenizer(ex[\"input_ids\"], return_tensors=\"pt\").input_ids\n",
    "        gen_ids = model.generate(input_ids, max_length=50, do_sample=False, num_beams=5)[0]\n",
    "        gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "        preds.append(gen_text.split())\n",
    "        refs.append([tokenizer.decode(ex[\"input_ids\"], skip_special_tokens=True).split()])\n",
    "    bleu = sacrebleu.corpus_bleu(preds, refs).score\n",
    "    return bleu\n",
    "\n",
    "bleu = eval_bleu()\n",
    "print(\"BLEU:\", bleu)\n",
    "with open(\"results/gpt2_bleu.json\", \"w\") as f:\n",
    "    json.dump({\"bleu\": bleu}, f, indent=2)\n",
    "\n",
    "# ⬛ GENERATE SAMPLE EXERCISE ⬛\n",
    "prompt = \"<prompt>Generate a fill‑in‑the‑blank exercise using common greetings in the language.<end>\"\n",
    "gen = model.generate(tokenizer(prompt, return_tensors=\"pt\").input_ids,\n",
    "                     max_len_\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ⬛ SET‑UP ⬛\n",
    "!pip install -q datasets transformers torchaudio jiwer accelerate\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import (Wav2Vec2ForCTC, Wav2Vec2Processor,\n",
    "                          TrainingArguments, Trainer)\n",
    "from jiwer import wer\n",
    "import torch, json, os, re\n",
    "\n",
    "DATA_DIR = \"data/raw/toy_lang_dataset\"       \n",
    "MANIFEST = f\"{DATA_DIR}/manifest_train.jsonl\"\n",
    "SAMPLE_RATE = 16_000\n",
    "MODEL_NAME = \"facebook/wav2vec2-base-960h\"\n",
    "\n",
    "# Helper: read JSONL → HF dataset\n",
    "def jsonl_to_dataset(path):\n",
    "    with open(path) as f:\n",
    "        items = [json.loads(l) for l in f]\n",
    "    ds = {\n",
    "        \"file\": [os.path.join(DATA_DIR, it[\"file\"]) for it in items],\n",
    "        \"transcription\": [it[\"transcription\"] for it in items]\n",
    "    }\n",
    "    ds = load_dataset(\"json\", data_files={\"train\": path})[\"train\"]\n",
    "    ds = ds.cast_column(\"file\", Audio(sampling_rate=SAMPLE_RATE))\n",
    "    ds = ds.train_test_split(test_size=0.2, seed=42)\n",
    "    return ds\n",
    "\n",
    "ds = jsonl_to_dataset(MANIFEST)\n",
    "\n",
    "# ⬛ TOKENIZER / PROCESSOR ⬛\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "def prepare(batch):\n",
    "    audio = batch[\"file\"][\"array\"]\n",
    "    batch[\"input_values\"] = processor(audio, sampling_rate=SAMPLE_RATE).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"transcription\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "ds = ds.map(prepare, remove_columns=ds[\"train\"].column_names, num_proc=2)\n",
    "\n",
    "# ⬛ MODEL + TRAINER ⬛\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"models/asr\",\n",
    "    per_device_train_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = torch.argmax(torch.tensor(pred.predictions), dim=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    return {\"wer\": wer(label_str, pred_str)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# ⬛ EVALUATE & SAVE ⬛\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "with open(\"results/asr_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "model.save_pretrained(\"models/asr\")\n",
    "processor.save_pretrained(\"models/asr\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
